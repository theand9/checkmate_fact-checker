{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QI9jhXKPCFcJ"
   },
   "source": [
    "# Stance Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSNgdEMpenpE"
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "1. Configure Data Path\n",
    "2. Load Embeddings\n",
    "3. Load Train and Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ly0VxAnwJ2f"
   },
   "source": [
    "### Configure Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmsPn6PF-cgL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(os.path.dirname(os.getcwd()), \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(f'{data_path}/external/glove.6B.zip', 'r') as z:\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29184,
     "status": "ok",
     "timestamp": 1575549472819,
     "user": {
      "displayName": "Swetha G",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBSOmXC9wxQRIbm4EyDIb4pKBa1_LiM40tURwHwlg=s64",
      "userId": "07021437028917822691"
     },
     "user_tz": -330
    },
    "id": "lsiKmyJUZ-hU",
    "outputId": "06dcd0c9-46a2-4be2-e733-3d97de6fe7c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/apollo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TjLJEQ_PwcGi"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gXO1WZ-gFwm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_bodies = pd.read_csv(f'{data_path}/stance/train/train_bodies.csv')\n",
    "train_stances = pd.read_csv(f'{data_path}/stance/train/train_stances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kosAWskdOOT8"
   },
   "outputs": [],
   "source": [
    "dataset = pd.merge(train_bodies[['Body ID','articleBody']],train_stances [['Body ID','Headline','Stance']],left_on = 'Body ID', right_on = 'Body ID', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1818,
     "status": "ok",
     "timestamp": 1575549433578,
     "user": {
      "displayName": "Swetha G",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBSOmXC9wxQRIbm4EyDIb4pKBa1_LiM40tURwHwlg=s64",
      "userId": "07021437028917822691"
     },
     "user_tz": -330
    },
    "id": "IUtF7iOmj11k",
    "outputId": "d68ada85-21c6-4de4-f5f6-e536022ba45b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody  \\\n",
       "0        0  A small meteorite crashed into a wooded area i...   \n",
       "1        0  A small meteorite crashed into a wooded area i...   \n",
       "2        0  A small meteorite crashed into a wooded area i...   \n",
       "3        0  A small meteorite crashed into a wooded area i...   \n",
       "4        0  A small meteorite crashed into a wooded area i...   \n",
       "\n",
       "                                            Headline     Stance  \n",
       "0  Soldier shot, Parliament locked down after gun...  unrelated  \n",
       "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated  \n",
       "2  Luke Somers 'killed in failed rescue attempt i...  unrelated  \n",
       "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated  \n",
       "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjzVz2ifijmj"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. Setting Hyperparameters\n",
    "2. Tokenization\n",
    "3. Encoding\n",
    "4. Convert Labels to One-Hot Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyperparameters\n",
    "\n",
    "- `MAX_SENTS` = Maximum no.of sentences to consider in an article.\n",
    "- `MAX_SENT_LENGTH` = Maximum no.of words to consider in a sentence.\n",
    "- `MAX_NB_WORDS` = Maximum no.of words in the total vocabualry.\n",
    "- `MAX_SENTS_HEADING` = Maximum no.of sentences to consider in a heading of an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDXSdpvqjuqw"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "MAX_SENTS = 20\n",
    "MAX_SENTS_HEADING = 1\n",
    "MAX_SENT_LENGTH = 20\n",
    "VALIDATION_SPLIT = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gqwm_GbwwnhX"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2977,
     "status": "ok",
     "timestamp": 1575549483487,
     "user": {
      "displayName": "Swetha G",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBSOmXC9wxQRIbm4EyDIb4pKBa1_LiM40tURwHwlg=s64",
      "userId": "07021437028917822691"
     },
     "user_tz": -330
    },
    "id": "S-VUgh2yoMlR",
    "outputId": "2fb3a316-b466-4706-c222-77e346c39db3"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qm85qirPofc2"
   },
   "outputs": [],
   "source": [
    "t = Tokenizer(num_words = MAX_NB_WORDS,filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}\\n\"~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5rk-UyBlmyA"
   },
   "outputs": [],
   "source": [
    "t.fit_on_texts(dataset['articleBody'])\n",
    "t.fit_on_texts(dataset['Headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1825,
     "status": "ok",
     "timestamp": 1575549521175,
     "user": {
      "displayName": "Swetha G",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBSOmXC9wxQRIbm4EyDIb4pKBa1_LiM40tURwHwlg=s64",
      "userId": "07021437028917822691"
     },
     "user_tz": -330
    },
    "id": "8rYfKsa8_htl",
    "outputId": "2fbd87c3-4c1b-47d3-c66a-7c1873877732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27879\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctEu-d4c4EZs"
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "texts = []\n",
    "articles = []\n",
    "\n",
    "for idx in range(dataset['articleBody'].shape[0]):\n",
    "    text = dataset['articleBody'][idx]\n",
    "    texts.append(text)\n",
    "  \n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    articles.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A small meteorite crashed into a wooded area in Nicaragua\\'s capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city\\'s airport, the Associated Press reports. \\n\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \\nMurillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\\n\\nThe crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\\n\\nHumberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\\n\\n\"We have to study it more because it could be ice or rock,\" he said.\\n\\nWilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\\n\\nLocal residents reported hearing a loud boom Saturday night, but said they didn\\'t see anything strange in the sky.\\n\\n\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\\n\\nThe site of the crater is near Managua\\'s international airport and an air force base. Only journalists from state media were allowed to visit it.',\n",
       " [\"A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday.\",\n",
       "  \"Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports.\",\n",
       "  'Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\"',\n",
       "  'House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports.',\n",
       "  'Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened.',\n",
       "  'The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee.',\n",
       "  'He said it is still not clear if the meteorite disintegrated or was buried.',\n",
       "  'Humberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.',\n",
       "  '\"We have to study it more because it could be ice or rock,\" he said.',\n",
       "  'Wilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light.',\n",
       "  'We have to ask if anyone has a photo or something.\"',\n",
       "  \"Local residents reported hearing a loud boom Saturday night, but said they didn't see anything strange in the sky.\",\n",
       "  '\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast.',\n",
       "  'We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.',\n",
       "  \"The site of the crater is near Managua's international airport and an air force base.\",\n",
       "  'Only journalists from state media were allowed to visit it.'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0], articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpuRIA7cCfcY"
   },
   "source": [
    "### Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1WoPAF2jEqlawIRiiR4wlSFUUs9Ww-MHQ"
    },
    "colab_type": "code",
    "id": "YVyClBULCqWj",
    "outputId": "bd9b5963-134c-4cf2-d9ac-22be010c06f2"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype = 'int32')\n",
    "for i, sentences in enumerate(articles):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "        for word in wordTokens:\n",
    "            try:\n",
    "                if k < MAX_SENT_LENGTH and t.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i,j,k] = t.word_index[word]\n",
    "                    k += 1\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsFWW5C2Djog"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    3,   481,   427,  7211,    81,     3,  3733,   331,     5,\n",
       "         3891,   350,     4,  1431,  2958,     1,    89,    12,   464,\n",
       "            0,     0],\n",
       "       [  758,    95,  1047,     3,  2679,  1752,     7,   189,     3,\n",
       "         1217,  1075,  2030,   700,   159,     1,  3032,   448,     1,\n",
       "          555,   235],\n",
       "       [   89,  1067,  4115,  2349,    12,     3,  1092,  3306,    19,\n",
       "            1,    89,     2,  1793,     1,   521,  2009,    15,     9,\n",
       "            3,  3111],\n",
       "       [  181,  3640,   972,   200,  2556,    44,  6775,  1722,  1252,\n",
       "            5, 13319, 17939,     1,   778,    31,   740,  3990,    67,\n",
       "           85,     0],\n",
       "       [ 2349,    12,  1557,    38,  1094,   351,   775,     2,   367,\n",
       "          260,  1770,     5,  4450,    70,   494,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [    1,   700,   189,    19,     1,   427,    32,     3,  7417,\n",
       "            4,  2159,  1252,     6,     3,  5270,     4,  1217,  1252,\n",
       "           12,  3363],\n",
       "       [   13,    12,    15,     8,   149,    25,   543,    64,     1,\n",
       "          427,  3727,    41,     9,  1850,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [ 3363,  5733,     4,     1,  5875,   614,    21,     1,   311,\n",
       "         3438,   793,     4,  1557,    12,     1,   427,    69,    23,\n",
       "          821,     2],\n",
       "       [   37,    17,     2,  1793,    15,    52,   120,    15,    69,\n",
       "           23,  4921,    41,  1963,    13,    12,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [ 4736,  3338,    24,  3969,     2,     1,  1316,     4,  3072,\n",
       "         1653,    12,    15,     9,   195,  1420,     7,    58,    40,\n",
       "           95,     3],\n",
       "       [   37,    17,     2,  1094,    64,   510,    20,     3,   250,\n",
       "           41,   264,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  260,   758,    95,  1047,     3,  1806,  1752,   531,   276,\n",
       "           29,    12,    33,   703,   163,   892,  1420,     5,     1,\n",
       "         2081,     0],\n",
       "       [   35,     9,  2057,    10,   116,  5825,     6,    35,   576,\n",
       "          656,   104,    59,     4,     3,  2410,    35,   241,     3,\n",
       "          512,  1911],\n",
       "       [   37,   341,    15,     9,     3,  2082,   120,    37,   880,\n",
       "           24,  4451,  2584,  4315,  4922,    55,     1,   555,   235,\n",
       "            0,     0],\n",
       "       [    1,   255,     4,     1,   700,     8,   159,  3961,   351,\n",
       "          448,     6,    24,   155,   465,  1929,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  126,   921,    22,    47,   100,    36,  1833,     2,  1212,\n",
       "           15,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTG6JySHehkT"
   },
   "source": [
    "### Header Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CliiIhLemJV"
   },
   "outputs": [],
   "source": [
    "text_heading = []\n",
    "articles_heading = []\n",
    "\n",
    "for idx in range(dataset['Headline'].shape[0]):\n",
    "    text = dataset['Headline'][idx]\n",
    "    text_heading.append(text)\n",
    "    \n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    articles_heading.append(sentences)\n",
    "\n",
    "data_heading = np.zeros((len(texts),MAX_SENTS_HEADING,MAX_SENT_LENGTH),dtype = 'int32')\n",
    "\n",
    "for i,sentences in enumerate(articles_heading):\n",
    "    for j,sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS_HEADING:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _,word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    if k < MAX_SENT_LENGTH and t.word_index[word] < MAX_NB_WORDS:\n",
    "                        data_heading[ i , j , k] = t.word_index[word]\n",
    "                        k += 1\n",
    "                except :\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaH0Ey1qe_Co"
   },
   "source": [
    "### Convert labels to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zq-VcgM8fat1"
   },
   "outputs": [],
   "source": [
    "labels = dataset['Stance']\n",
    "targets = pd.Series(labels)\n",
    "\n",
    "one_hot = pd.get_dummies(targets,sparse = True)\n",
    "one_hot_labels = np.asarray(one_hot)\n",
    "\n",
    "labels = one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vpEWEnjFfnFR",
    "outputId": "312006c9-6dff-4704-978e-d08131b483c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (49972, 20, 20)\n",
      "Shape of label tensor: (49972, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDOxHdR3frDu"
   },
   "source": [
    "### Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ra-yYTvfzRt"
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "data_heading = data_heading[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JcOFVfPBf9kA"
   },
   "source": [
    "### Split data into train and validation set (80:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o5u3PTz3gEV-"
   },
   "outputs": [],
   "source": [
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "x_heading_train = data_heading[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "\n",
    "x_val = data[-nb_validation_samples:]\n",
    "x_heading_val = data_heading[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "KLEbiw2Yghe2",
    "outputId": "e2390c42-fcda-4165-ae0e-52bd515d8b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37479, 20, 20)\n",
      "(37479, 4)\n",
      "(12493, 20, 20)\n",
      "(12493, 4)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNnoBtArhJ1E"
   },
   "source": [
    "### Create embedding matrix with the glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eKqn2IL2ZF8v",
    "outputId": "076e80d8-5a98-434e-eb51-a5f02f532c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "\n",
    "\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LRi4o3ZspDFU"
   },
   "source": [
    "## ML Model\n",
    "\n",
    "1. Imports\n",
    "1. Model Design\n",
    "2. Model Fitting on Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zSZDnPWkw2ZZ"
   },
   "source": [
    "### Import layers from Keras to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AgwQsfMrzAQ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM, TimeDistributed, Activation\n",
    "from keras.layers import Flatten, Permute, merge, Input\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, multiply, concatenate, Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpkVhIbx3gr1"
   },
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_8QXh-rmPFq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1:0\", shape=(None, 20), dtype=int32)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Tensor(\"input_2:0\", shape=(None, 20, 20), dtype=int32)\n",
      "Tensor(\"time_distributed_1/Reshape_1:0\", shape=(None, 20, 2000), dtype=float32)\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LENGTH = 20\n",
    "MAX_SENTS = 20\n",
    "hidden_size = 100\n",
    "\n",
    "from keras.layers import GRU, Bidirectional\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,),dtype='int32')\n",
    "print(sentence_input)\n",
    "embedded_sequences = Embedding(output_dim = hidden_size, input_dim = vocab_size, input_length = (MAX_SENT_LENGTH,),)(sentence_input)\n",
    "\n",
    "l_LSTM = Bidirectional(LSTM(100,return_sequences = True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(100))(l_LSTM)\n",
    "l_dense = Flatten()(l_dense)\n",
    "sentEncoder = Model(sentence_input,l_dense)\n",
    "\n",
    "body_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH,),dtype = 'int32')\n",
    "print(body_input)\n",
    "body_encoder = TimeDistributed(sentEncoder)(body_input)\n",
    "print(body_encoder)\n",
    "l_LSTM_sent = Bidirectional(LSTM(100,return_sequences=True))(body_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(100))(l_LSTM_sent)\n",
    "l_dense_sent = Flatten()(l_dense_sent)\n",
    "\n",
    "heading_input = Input(shape = (MAX_SENTS_HEADING, MAX_SENT_LENGTH),dtype = 'int32')\n",
    "heading_embedded_sequences = Embedding(output_dim=hidden_size, input_dim=vocab_size, \\\n",
    "                                       input_length = (MAX_SENTS_HEADING,MAX_SENT_LENGTH,), \\\n",
    "                                      weights = [embedding_matrix])(heading_input)\n",
    "h_dense = Dense(100,activation='relu')(heading_embedded_sequences)\n",
    "h_flatten = Flatten()(h_dense)\n",
    "article_output = concatenate([l_dense_sent,h_flatten],name = 'concatenate_heading')\n",
    "\n",
    "news_vestor = Dense(100,activation = 'relu')(article_output)\n",
    "preds = Dense(4,activation = 'softmax')(news_vestor)\n",
    "model = Model([body_input,heading_input],[preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5Xrd-JQ3id7"
   },
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CM3yCmjQoCM3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting = Hierarchical LSTM network\n",
      "Train on 37479 samples, validate on 12493 samples\n",
      "Epoch 1/10\n",
      "37479/37479 [==============================] - ETA: 0s - loss: 0.5755 - acc: 0.7912"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apollo/miniconda3/envs/fact_check/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37479/37479 [==============================] - 88s 2ms/sample - loss: 0.5755 - acc: 0.7912 - val_loss: 0.4065 - val_acc: 0.8453\n",
      "Epoch 2/10\n",
      "37479/37479 [==============================] - 92s 2ms/sample - loss: 0.3049 - acc: 0.8823 - val_loss: 0.2882 - val_acc: 0.8934\n",
      "Epoch 3/10\n",
      "37479/37479 [==============================] - 93s 2ms/sample - loss: 0.1928 - acc: 0.9257 - val_loss: 0.2262 - val_acc: 0.9167\n",
      "Epoch 4/10\n",
      "37479/37479 [==============================] - 93s 2ms/sample - loss: 0.1329 - acc: 0.9493 - val_loss: 0.2079 - val_acc: 0.9273\n",
      "Epoch 5/10\n",
      "37479/37479 [==============================] - 93s 2ms/sample - loss: 0.1010 - acc: 0.9630 - val_loss: 0.2099 - val_acc: 0.9328\n",
      "Epoch 6/10\n",
      "37479/37479 [==============================] - 92s 2ms/sample - loss: 0.0784 - acc: 0.9709 - val_loss: 0.1794 - val_acc: 0.9458\n",
      "Epoch 7/10\n",
      "37479/37479 [==============================] - 92s 2ms/sample - loss: 0.0576 - acc: 0.9785 - val_loss: 0.2017 - val_acc: 0.9448\n",
      "Epoch 8/10\n",
      "37479/37479 [==============================] - 92s 2ms/sample - loss: 0.0540 - acc: 0.9806 - val_loss: 0.1861 - val_acc: 0.9488\n",
      "Epoch 9/10\n",
      "37479/37479 [==============================] - 92s 2ms/sample - loss: 0.0411 - acc: 0.9854 - val_loss: 0.2125 - val_acc: 0.9460\n",
      "Epoch 10/10\n",
      "37479/37479 [==============================] - 92s 2ms/sample - loss: 0.0406 - acc: 0.9859 - val_loss: 0.2154 - val_acc: 0.9497\n",
      "INFO:tensorflow:Assets written to: /home/apollo/Desktop/DJSCE/Projects/checkmate_fact_checker/src/models/assets\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['acc'])\n",
    "print(\"model fitting = Hierarchical LSTM network\")\n",
    "model.fit([x_train,x_heading_train], [y_train], validation_data = ([x_val, x_heading_val], [y_val]), epochs = 10 ,batch_size=62)\n",
    "\n",
    "model.save(os.path.join(os.path.dirname(os.getcwd()), \"src/models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Stance_Detection_for_the_Fake_News_Challenge_Questions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
